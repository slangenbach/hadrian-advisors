{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing : Introduction to Word2Vec\n",
    "# Using Gensim API ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction to Gensim Models and Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypath = os.path.abspath(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GIT\\\\hadrian-advisors\\\\data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "input_loc = mypath + '/' + 'text8'\n",
    "sentences = word2vec.Text8Corpus(input_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genmodel = word2vec.Word2Vec(sentences, min_count=10, size=100)\n",
    "#Declare our input location, generate our model. We're using the same parameters as before.\n",
    "query_words = genmodel.most_similar(positive=['emperor','woman'], negative=['man'], topn=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('empress', 0.701883852481842)\n('ruler', 0.6259490847587585)\n('emperors', 0.6234719157218933)\n('consul', 0.6019366979598999)\n('dynasty', 0.5975302457809448)\n('nero', 0.5931859016418457)\n('augustus', 0.59293532371521)\n('constantine', 0.5897301435470581)\n('throne', 0.5871866345405579)\n('charlemagne', 0.5732648372650146)\n"
     ]
    }
   ],
   "source": [
    "for word in query_words[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ruler', 0.7831838726997375),\n ('constantine', 0.7570105791091919),\n ('empress', 0.748015284538269),\n ('augustus', 0.7300704717636108),\n ('emperors', 0.7300524115562439),\n ('sultan', 0.7192990779876709),\n ('king', 0.7104095220565796),\n ('justinian', 0.7094637155532837),\n ('pope', 0.6962057948112488),\n ('constantius', 0.688994288444519)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genmodel.most_similar(positive='emperor', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84491754 -0.81446719  0.10439272  0.51581478 -0.79089725  0.84258777\n  0.5803346   0.46511164 -1.82057989  1.18477631  2.01468587 -0.00480545\n  0.53168786  0.17571875  0.04555919  0.91794509  0.36345974 -0.28167513\n  0.58821869  0.56821746 -1.19191158  2.31244063 -0.20456545  0.50960118\n -0.67754471  1.39947712 -0.85569489 -0.20671386 -0.64269549 -0.9545635\n  0.66066265  2.02892137  0.00703002 -0.24611543 -0.12673263  1.04394138\n -1.4514209  -0.37382269 -0.33639115 -1.86846662 -0.33613917 -0.35964128\n -0.10061048  0.78780413 -0.69861537  1.15342426 -0.21812618  1.10869575\n  1.04718101  0.79547268  0.13696316 -0.40387824 -0.70576686  0.18860994\n  0.33833081 -0.69339943 -0.35302645 -0.9362008   0.62110454 -0.12615733\n  1.15475535 -1.87328768  0.89216888 -0.52497983  0.04575916  0.69227904\n -0.39583793  0.26428014  0.6704933  -0.00954044 -1.019153    0.62290281\n  0.24006282 -1.39001119 -0.20130546  0.01460918 -0.20458929  0.03572461\n -0.05062975 -0.7529577   0.08417737  0.14175904  0.77594018  0.11050501\n  0.11661911  0.25431097 -0.16064657 -0.66142696  0.90611762  0.62542129\n -0.05604555  0.41218561  2.6363194   0.53047729 -0.37190282  0.35417184\n  0.71421897 -0.03969391 -0.00854268 -0.78547049]\n"
     ]
    }
   ],
   "source": [
    "# get the word vector of \"the\"\n",
    "print(genmodel.wv['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the of and\n"
     ]
    }
   ],
   "source": [
    "# get the word vector of \"the\"\n",
    "print(genmodel.wv.index2word[0], genmodel.wv.index2word[1], genmodel.wv.index2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejuvenation moorehead monophysites\n"
     ]
    }
   ],
   "source": [
    "# get the least common words\n",
    "vocab_size = len(genmodel.wv.vocab)\n",
    "print(genmodel.wv.index2word[vocab_size - 1], genmodel.wv.index2word[vocab_size - 2], genmodel.wv.index2word[vocab_size - 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of \"of\" is: 1\n"
     ]
    }
   ],
   "source": [
    "# find the index of the 2nd most common word (\"of\")\n",
    "print('Index of \"of\" is: {}'.format(genmodel.wv.vocab['of'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750751857304 0.302815564191\n"
     ]
    }
   ],
   "source": [
    "# some similarity functions\n",
    "print(genmodel.wv.similarity('woman', 'man'), genmodel.wv.similarity('man', 'elephant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zebra\n"
     ]
    }
   ],
   "source": [
    "# what doesn't fit?\n",
    "print(genmodel.wv.doesnt_match(\"green blue red zebra\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and reload the model\n",
    "genmodel.save(mypath + \"mymodel\")\n",
    "#genmodel = gensim.models.Word2Vec.load(mypath + \"mymodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II. Example of Wrong Word2Vec training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To train on a classical file and not **Text8**, the input of Word2Vec must be an iterator object; We can use the class below to learn on a file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname, filename):\n",
    "        self.dirname = dirname\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(os.path.join(self.dirname, self.filename)):\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GIT\\\\hadrian-advisors\\\\data\\\\ExamplePasta.txt'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(mypath, 'ExamplePasta.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentencesPasta = MySentences(mypath,'ExamplePasta.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ExamplePasta.txt'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesPasta.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GIT\\\\hadrian-advisors\\\\data'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesPasta.dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentencesPasta, min_count=1, size=5, sg=0,cbow_mean=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': <gensim.models.keyedvectors.Vocab at 0x76cd828>,\n ':': <gensim.models.keyedvectors.Vocab at 0x76cd908>,\n 'America': <gensim.models.keyedvectors.Vocab at 0x768b6a0>,\n 'Cannelloni,': <gensim.models.keyedvectors.Vocab at 0x76cdef0>,\n 'Farfalle,': <gensim.models.keyedvectors.Vocab at 0x76cd3c8>,\n 'Fettuccine,': <gensim.models.keyedvectors.Vocab at 0x76cd4e0>,\n 'Fusili,': <gensim.models.keyedvectors.Vocab at 0x76cd860>,\n 'Hamburgers': <gensim.models.keyedvectors.Vocab at 0x768b278>,\n 'Hamburgers.': <gensim.models.keyedvectors.Vocab at 0x768b1d0>,\n 'I': <gensim.models.keyedvectors.Vocab at 0x76cdc50>,\n \"It's\": <gensim.models.keyedvectors.Vocab at 0x768bb38>,\n 'Linguine,': <gensim.models.keyedvectors.Vocab at 0x76cdeb8>,\n 'Penne,': <gensim.models.keyedvectors.Vocab at 0x76cd5f8>,\n 'Rigatoni': <gensim.models.keyedvectors.Vocab at 0x76cd940>,\n 'Spaghetti,': <gensim.models.keyedvectors.Vocab at 0x76cdc88>,\n 'Tagliatelle,': <gensim.models.keyedvectors.Vocab at 0x76cd048>,\n 'Tortellini': <gensim.models.keyedvectors.Vocab at 0x76cd5c0>,\n 'and': <gensim.models.keyedvectors.Vocab at 0xe94bf98>,\n 'better': <gensim.models.keyedvectors.Vocab at 0x76cd278>,\n 'different': <gensim.models.keyedvectors.Vocab at 0x76cd7b8>,\n 'in': <gensim.models.keyedvectors.Vocab at 0x76cd630>,\n 'many': <gensim.models.keyedvectors.Vocab at 0xd7960f0>,\n 'much': <gensim.models.keyedvectors.Vocab at 0x76cd400>,\n 'of': <gensim.models.keyedvectors.Vocab at 0x76cdfd0>,\n 'once': <gensim.models.keyedvectors.Vocab at 0x768b4a8>,\n 'pastas': <gensim.models.keyedvectors.Vocab at 0x76cd588>,\n 'so': <gensim.models.keyedvectors.Vocab at 0x76cd780>,\n 'than': <gensim.models.keyedvectors.Vocab at 0x76cdf60>,\n 'then': <gensim.models.keyedvectors.Vocab at 0x76cdac8>,\n 'tried': <gensim.models.keyedvectors.Vocab at 0x76cd7f0>,\n 'type': <gensim.models.keyedvectors.Vocab at 0x76cd978>}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_words = model.most_similar(positive=['Hamburgers'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('better', 0.8504067063331604), ('Fettuccine,', 0.8257306218147278), ('of', 0.7704744338989258), ('so', 0.6697015166282654), ('many', 0.5907824635505676)]\n"
     ]
    }
   ],
   "source": [
    "print(query_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Example : Sentiment analysis using Word2Vec\n",
    "## Comparison with Classic methods (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunten\\AppData\\Local\\Continuum\\Anaconda3\\envs\\hadrian-advisors\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = mypath + '\\\\Kaggle\\\\'\n",
    "filename = 'labeledTrainData.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>12205_8</td>\n",
       "      <td>1</td>\n",
       "      <td>After CITIZEN KANE in 1941, Hollywood executiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>8224_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching this Movie? l thought to myself, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>2596_10</td>\n",
       "      <td>1</td>\n",
       "      <td>The film adaptation of James Joyce's Ulysses i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>412_4</td>\n",
       "      <td>0</td>\n",
       "      <td>Within the realm of Science Fiction, two parti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>7934_7</td>\n",
       "      <td>1</td>\n",
       "      <td>According to this board, I guess either you lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>12205_8</td>\n",
       "      <td>1</td>\n",
       "      <td>After CITIZEN KANE in 1941, Hollywood executiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>8224_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching this Movie? l thought to myself, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>2596_10</td>\n",
       "      <td>1</td>\n",
       "      <td>The film adaptation of James Joyce's Ulysses i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>412_4</td>\n",
       "      <td>0</td>\n",
       "      <td>Within the realm of Science Fiction, two parti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>7934_7</td>\n",
       "      <td>1</td>\n",
       "      <td>According to this board, I guess either you lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(pathData, nb_rows, propVal):\n",
    "    data_all = pd.read_table(pathData,header=0,nrows=nb_rows,\n",
    "                            error_bad_lines=False)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_val = train_test_split(data_all, test_size = propVal)\n",
    "    return data_train, data_val\n",
    "\n",
    "nb_rows=100000\n",
    "propVal=0.20\n",
    "data_train, data_val = split_dataset(os.path.join(dirname, filename),\n",
    "                                     nb_rows, propVal)\n",
    "\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import re # Regex\n",
    "import nltk # Cleaning\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata \n",
    "\n",
    "## Stop words\n",
    "## Using NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english') \n",
    "stopwords = list(nltk_stopwords)\n",
    "\n",
    "## Stemming function to get the root\n",
    "stemmer=nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General cleaning function\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore')\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt.decode('utf-8'))\n",
    "    ### remove english stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### english stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning function (stemming and Stop words)\n",
    "def clean_df(input_data, column_names= ['review']):\n",
    "    #Test if columns entry match columns names of input data\n",
    "    column_names_diff = set(column_names).difference(set(input_data.columns))\n",
    "    if column_names_diff:\n",
    "        warnings.warn(\"Column(s) '\"+\", \".join(list(column_names_diff)) +\"' do(es) not match columns of input data\", Warning)\n",
    "    \n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name]\n",
    "        new_clean = column.apply(lambda elt: clean_txt(elt))\n",
    "        #array_clean = np.array(map(clean_txt,column))\n",
    "        clean_list.append(new_clean)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    #array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.concat(clean_list,axis=1)\n",
    "    #data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 5000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunten\\AppData\\Local\\Continuum\\Anaconda3\\envs\\hadrian-advisors\\lib\\site-packages\\bs4\\__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 18 secondes\nStart Clean 20000 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 63 secondes\n"
     ]
    }
   ],
   "source": [
    "# Take approximately 2 minutes for 100.000 rows\n",
    "data_valid_clean = clean_df(data_val)\n",
    "data_train_clean = clean_df(data_train)\n",
    "data_train_clean['id'] = data_train['id'].values\n",
    "data_valid_clean['id'] = data_val['id'].values\n",
    "data_train_clean['sentiment'] = data_train['sentiment'].values\n",
    "data_valid_clean['sentiment'] = data_val['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>12205_8</td>\n",
       "      <td>1</td>\n",
       "      <td>After CITIZEN KANE in 1941, Hollywood executiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>8224_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching this Movie? l thought to myself, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>2596_10</td>\n",
       "      <td>1</td>\n",
       "      <td>The film adaptation of James Joyce's Ulysses i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>412_4</td>\n",
       "      <td>0</td>\n",
       "      <td>Within the realm of Science Fiction, two parti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>7934_7</td>\n",
       "      <td>1</td>\n",
       "      <td>According to this board, I guess either you lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>12205_8</td>\n",
       "      <td>1</td>\n",
       "      <td>After CITIZEN KANE in 1941, Hollywood executiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>8224_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching this Movie? l thought to myself, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>2596_10</td>\n",
       "      <td>1</td>\n",
       "      <td>The film adaptation of James Joyce's Ulysses i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>412_4</td>\n",
       "      <td>0</td>\n",
       "      <td>Within the realm of Science Fiction, two parti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>7934_7</td>\n",
       "      <td>1</td>\n",
       "      <td>According to this board, I guess either you lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>citizen kane hollywood execut turn cob web bac...</td>\n",
       "      <td>12205_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>watch movi thought lot garbag girl must rock b...</td>\n",
       "      <td>8224_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>film adapt jame joyc ulyss excel actor voic ov...</td>\n",
       "      <td>2596_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>within realm scienc fiction two particular the...</td>\n",
       "      <td>412_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>accord board guess either love hate usual goe ...</td>\n",
       "      <td>7934_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>citizen kane hollywood execut turn cob web bac...</td>\n",
       "      <td>12205_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>watch movi thought lot garbag girl must rock b...</td>\n",
       "      <td>8224_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>film adapt jame joyc ulyss excel actor voic ov...</td>\n",
       "      <td>2596_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>within realm scienc fiction two particular the...</td>\n",
       "      <td>412_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>accord board guess either love hate usual goe ...</td>\n",
       "      <td>7934_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>mani thing fall aro tolbukhin ment del asesino...</td>\n",
       "      <td>10024_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22767</th>\n",
       "      <td>movi fall well standard ultim answer lie poor ...</td>\n",
       "      <td>6328_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>far one favorit american pie spin off main oth...</td>\n",
       "      <td>8552_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>think ebert gave stella four four star never r...</td>\n",
       "      <td>2990_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23851</th>\n",
       "      <td>mean serious group would sing crazi car ten wa...</td>\n",
       "      <td>4698_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>mani thing fall aro tolbukhin ment del asesino...</td>\n",
       "      <td>10024_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22767</th>\n",
       "      <td>movi fall well standard ultim answer lie poor ...</td>\n",
       "      <td>6328_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>far one favorit american pie spin off main oth...</td>\n",
       "      <td>8552_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>think ebert gave stella four four star never r...</td>\n",
       "      <td>2990_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23851</th>\n",
       "      <td>mean serious group would sing crazi car ten wa...</td>\n",
       "      <td>4698_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean.to_csv(mypath+'\\\\cleanedTrainData.csv',index=None,header=None)\n",
    "data_valid_clean.to_csv(mypath+'\\\\cleanedValidData.csv',index=None,header=None)\n",
    "\n",
    "## merge train set and validation set for the learning\n",
    "data_train_clean['id'] = data_train_clean['id'].apply(lambda elt: 'train_' + str(elt))\n",
    "data_valid_clean['id'] = data_valid_clean['id'].apply(lambda elt: 'val_' + str(elt))\n",
    "data_clean = pd.concat([data_train_clean,data_valid_clean],axis=0)\n",
    "data_clean.to_csv(mypath+'\\\\cleanedData.csv',index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                  review             id  \\\n 10009  citizen kane hollywood execut turn cob web bac...  train_12205_8   \n 16950  watch movi thought lot garbag girl must rock b...   train_8224_1   \n 12554  film adapt jame joyc ulyss excel actor voic ov...  train_2596_10   \n 17175  within realm scienc fiction two particular the...    train_412_4   \n 15076  accord board guess either love hate usual goe ...   train_7934_7   \n \n        sentiment  \n 10009          1  \n 16950          0  \n 12554          1  \n 17175          0  \n 15076          1  ,\n                                                   review           id  \\\n 5574   far one worst movi ever seen poor special effe...  val_10177_1   \n 23144  thirti year initi releas third version star bo...   val_7434_7   \n 7874   definit top five best john garfield movi pride...   val_7114_9   \n 9593   noth new hackney romanc charact put unbeliev s...   val_5571_1   \n 24507  best cheech chong movi far cheech chong certai...  val_5564_10   \n \n        sentiment  \n 5574           0  \n 23144          1  \n 7874           1  \n 9593           0  \n 24507          1  )"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.head(), data_clean.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Word2Vec Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to use the **Doc2Vec** algorithm; it is very efficient in this case because it modifies the word2vec algorithm to unsupervised learning of continuous representations for larger blocks of text, such as sentences, paragraphs or entire documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrarily to the classical word2vec algorithm, the two choices of training algorithm that you have are **“distributed memory”** (dm) and **“distributed bag of words”** (dbow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the input of Doc2Vec is a LabeledSentence object; Each object represents a single sentence, and consists of two simple lists: a list of words and a list of labels. We can use the class below to learn on a file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDocuments(object):\n",
    "    def __init__(self, dirname, filename):\n",
    "        self.dirname = dirname\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(os.path.join(self.dirname, self.filename)):\n",
    "            yield doc2vec.TaggedDocument(line.split()[:-2],[line.split()[-1]])\n",
    "\n",
    "documents = MyDocuments(mypath,'cleanedData.csv')\n",
    "modelDoc = doc2vec.Doc2Vec(documents, size=300,\n",
    "                           min_count=10, window = 10, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDoc.save(mypath+'\\\\modelDoc2Vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TransformForUnseenDoc(modelDoc,data_valid_clean,column_names):\n",
    "    '''\n",
    "    Transforms Unseen documents into vectors using pre-trained model\n",
    "    '''\n",
    "    X_val = data_valid_clean[column_names].apply(\n",
    "                                lambda doc: modelDoc.infer_vector(doc))\n",
    "    X_val = np.vstack(X_val.values)\n",
    "    return X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's try our function on a validation sample\n",
    "X_val = TransformForUnseenDoc(modelDoc,data_valid_clean,'review')\n",
    "Y_val = data_valid_clean['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Here we have trained on all the documents at once **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the docs into vectors using modelDoc\n",
    "X = [docvec for docvec in modelDoc.docvecs]\n",
    "X = np.vstack(X)\n",
    "\n",
    "## Splitting our sets using the train_ids and val_ids\n",
    "X_train = X[['train_' in Id for Id in data_clean['id']]]\n",
    "X_val = X[['val_' in Id for Id in data_clean['id']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = data_train_clean['sentiment'].values\n",
    "Y_val = data_valid_clean['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 300), (5000, 300))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000,), (5000,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26831424, -0.02789249, -0.03168119, -0.06262571,  0.01448183],\n       [ 0.00370893, -0.03014431, -0.00281535,  0.02295889,  0.01300901],\n       [ 0.00306431, -0.00082015, -0.00722182,  0.03490874,  0.02076876],\n       [ 0.2167041 , -0.03764789, -0.1089885 , -0.04126954, -0.06098915],\n       [ 0.10300856, -0.05692465, -0.0019634 ,  0.03368974,  0.02153131]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Scikit-learn **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Takes 71 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training score : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "## estimation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=300,n_jobs=-2,max_features=25)\n",
    "time_start = time.time()\n",
    "rf = rf.fit(X_train, Y_train)\n",
    "time_end = time.time()\n",
    "print(\"RF Takes %d s\" %(time_end-time_start) )\n",
    "score=rf.score(X_train,Y_train)\n",
    "print('# training score :',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation score : 0.8136\n"
     ]
    }
   ],
   "source": [
    "scoreValidation=rf.score(X_val,Y_val)\n",
    "print('# validation score :',scoreValidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creation of a matrix showing\n",
    "## the frequencies of the words contained in each review\n",
    "## many parameters to test\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "\n",
    "def vectorizer_train(df, columns=['review'], nb_hash=None, stop_words=None):\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "        feathash = None\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(map(collections.Counter,df_text))\n",
    "\n",
    "    # TFIDF\n",
    "    vec = TfidfVectorizer(\n",
    "        min_df = 1,\n",
    "        stop_words = stop_words,\n",
    "        smooth_idf=True,\n",
    "        norm='l2',\n",
    "        sublinear_tf=True,\n",
    "        use_idf=True,\n",
    "        ngram_range=(1,2)) #bi-grams\n",
    "    tfidf = vec.fit_transform(data_hash)\n",
    "    return vec, feathash, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_vectorizer(df, vec, columns =['review'], feathash = None ):\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        data_hash = feathash.transform(df_text)\n",
    "    \n",
    "    # TFIDF\n",
    "    tfidf=vec.transform(data_hash)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec, feathash, tf_X_train = vectorizer_train(data_train_clean)\n",
    "\n",
    "tf_X_val = apply_vectorizer(data_valid_clean, vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 1302478), (20000, 1302478))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_X_val.shape, tf_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x5 sparse matrix of type '<class 'numpy.float64'>'\n\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_X_train[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Scikit-learn **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Takes 375 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training score : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "## estimation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=300,n_jobs=-2,max_features=25)\n",
    "time_start = time.time()\n",
    "rf = rf.fit(tf_X_train, Y_train)\n",
    "time_end = time.time()\n",
    "print(\"RF Takes %d s\" %(time_end-time_start) )\n",
    "score=rf.score(tf_X_train,Y_train)\n",
    "print('# training score :',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation score : 0.8676\n"
     ]
    }
   ],
   "source": [
    "scoreValidation=rf.score(tf_X_val,Y_val)\n",
    "print('# validation score :',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Using KERAS with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_keras_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypath = os.path.abspath('./')\n",
    "pathTwitter = os.path.join(mypath,'Twitter\\\\dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "nRows = 50000\n",
    "\n",
    "# Parse tweets and sentiments\n",
    "with open(pathTwitter, 'r', encoding='utf-8') as df:\n",
    "    for i, line in enumerate(df):\n",
    "        if i >= nRows+1:\n",
    "            break;\n",
    "        if i == 0:\n",
    "            # Skip the header\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split(',')\n",
    "        \n",
    "        # Sentiment (0 = Negative, 1 = Positive)\n",
    "        labels.append(int(parts[1].strip()))\n",
    "        \n",
    "        # Tweet\n",
    "        tweet = parts[3].strip()\n",
    "        if tweet.startswith('\"'):\n",
    "            tweet = tweet[1:]\n",
    "        if tweet.endswith('\"'):\n",
    "            tweet = tweet[::-1]\n",
    "        \n",
    "        corpus.append(tweet.strip().lower())\n",
    "        \n",
    "print('Corpus size: {}'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and stem\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9@]+')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for i, tweet in enumerate(corpus):\n",
    "    tokens = [stemmer.stem(t) for t in tkr.tokenize(tweet) if not t.startswith('@')]\n",
    "    tokenized_corpus.append(tokens)\n",
    "    \n",
    "# Gensim Word2Vec model\n",
    "vector_size = 512\n",
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Word2Vec\n",
    "modelTwitter = Word2Vec(sentences=tokenized_corpus,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=-2)\n",
    "\n",
    "# Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "X_vecs = modelTwitter.wv\n",
    "del modelTwitter\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train subset size (0 < size < len(tokenized_corpus))\n",
    "train_size = 40000\n",
    "\n",
    "# Test subset size (0 < size < len(tokenized_corpus) - train_size)\n",
    "test_size = 10000\n",
    "\n",
    "# Compute average and max tweet length\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tokenized_corpus:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tokenized_corpus))))\n",
    "print('Max tweet length: {}'.format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tweet max length (number of tokens)\n",
    "max_tweet_length = 15\n",
    "\n",
    "# Create train and test sets\n",
    "# Generate random indexes\n",
    "indexes = set(np.random.choice(len(tokenized_corpus), train_size + test_size, replace=False))\n",
    "\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((test_size, 2), dtype=np.int32)\n",
    "\n",
    "for i, index in enumerate(indexes):\n",
    "    for t, token in enumerate(tokenized_corpus[index]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n",
    "            \n",
    "    if i < train_size:\n",
    "        Y_train[i, :] = [1.0, 0.0] if labels[index] == 0 else [0.0, 1.0]\n",
    "    else:\n",
    "        Y_test[i - train_size, :] = [1.0, 0.0] if labels[index] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras convolutional model\n",
    "batch_size = 100\n",
    "nb_epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_tweet_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[EarlyStopping(min_delta=0.0025, patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluation = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
